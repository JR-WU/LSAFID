from typing import List, Dict, Tuple, Callable
from numpy import newaxis, arange, ndarray, zeros

from ..corpus import Corpus

TuplesT = Tuple[Tuple[str, float], ...]


class PlsaResult:
    """Container for the results generated by a (conditional) PLSA run.

    Parameters
    ----------
    topic_given_doc: ndarray
        The conditional probability `p(t|d)` as
        :math:`n_{topics}\\times n_{docs}` array.
    word_given_topic: ndarray
        The conditional probability `p(w|t)` as
        :math:`n_{words}\\times n_{topics}` array.
    topic_given_word: ndarray
        The conditional probability `p(t|w)` as
        :math:`n_{topics}\\times n_{words}` array.
    topic: ndarray
        The marginal probability `p(w)`.
    kl_divergences: list of float
        The Kullback-Leibler divergences between the original document-word
        probability `p(d, w)` and its approximate for each iteration.
    corpus: Corpus
        The original corpus the PLSA model was trained on.
    tf_idf: bool
        Whether to weigh the document.word matrix with the inverse document
        frequencies or not.

    """
    def __init__(self, topic_given_doc: ndarray,
                 word_given_topic: ndarray,
                 topic_given_word: ndarray,
                 topic: ndarray,
                 kl_divergences: List[float],
                 corpus: Corpus,
                 tf_idf: bool) -> None:
        self.__n_topics = topic.size
        self.__kl_divergences = kl_divergences
        self.__corpus = corpus
        self.__tf_idf = tf_idf
        self.__topic, topic_order = self.__ordered_topic(topic)
        self.__topic_given_doc = topic_given_doc[topic_order]
        self.__topic_given_word: ndarray = topic_given_word[topic_order]
        word_given_topic = word_given_topic[:, topic_order]
        word_given_topic, word_order = self.__sorted(word_given_topic)
        zipped = self.__zipped(word_order, word_given_topic)
        tuples = self.__tuples(corpus.vocabulary)
        topics = range(topic.size)
        self.__word_given_topic = tuple(tuples(zipped(t)) for t in topics)

    def __repr__(self) -> str:
        title = self.__class__.__name__
        header = f'{title}:\n'
        divider = '=' * len(title) + '\n'
        n_topics = f'Number of topics:    {self.__n_topics}\n'
        n_docs = f'Number of documents: {len(self.__topic_given_doc[0])}\n'
        n_words = f'Number of words:     {len(self.__word_given_topic[0])}'
        body = n_topics + n_docs + n_words
        return header + divider + body

    @property
    def n_topics(self) -> int:
        """The number of latent topics identified."""
        return self.__n_topics

    @property
    def tf_idf(self) -> bool:
        """Used inverse document frequency to weigh the document-word counts?"""
        return self.__tf_idf

    @property
    def kl_divergence(self) -> float:
        """KL-divergence of approximate and true document-word probability."""
        return self.__kl_divergences[-1]

    @property
    def topic(self) -> ndarray:
        """The relative importance of latent topics."""
        return self.__topic

    @property
    def word_given_topic(self) -> Tuple[TuplesT, ...]:
        """The words in each latent topic and their relative importance.

        Results are presented as a tuple of 2-tuples (word, word importance).

        """
        return self.__word_given_topic

    @property
    def topic_given_doc(self) -> ndarray:
        """The relative importance of latent topics in each document.

        Dimensions are :math:`n_{docs} \\times n_{topics}`.

        """
        return self.__topic_given_doc.T

    @property
    def convergence(self) -> List[float]:
        """The convergence of the Kullback-Leibler divergence."""
        return self.__kl_divergences

    def predict(self, doc: str) -> Tuple[ndarray, int, Tuple[str, ...]]:
        """Predict the relative importance of latent topics in a new document.

        Parameters
        ----------
        doc: str
            A new document given as a single string.

        Returns
        -------
        ndarray
            A 1-D array with the relative importance of latent topics.
        int
            The number of words in the new document that were not present in
            the corpus the PLSA model was trained on.
        tuple of str
            Those words in the new document that were not present in the
            corpus the PLSA model was trained on.

        Raises
        ------
        ValueError
            If the document to predict on is an empty string, if there are
            no words left after preprocessing the document, or if there are
            no known words in the document.

        """
        doc = self.__non_empty_string(doc)
        processed = self.__safely_processed(doc)
        encoded = zeros(self.__corpus.n_words + 1)
        new_words = []
        for word in processed:
            index = self.__corpus.index.get(word, self.__corpus.n_words)
            encoded[index] += 1
            if index == self.__corpus.n_words:
                new_words.append(word)
        encoded, n_new_words = self.__evaluated(encoded)
        encoded = encoded * self.__corpus.idf if self.__tf_idf else encoded
        encoded /= encoded.sum()
        new_words = tuple(new_words)
        return self.__topic_given_word.dot(encoded), n_new_words, new_words

    def __ordered_topic(self, topic: ndarray) -> (ndarray, ndarray):
        """Sort topics by their relative importance."""
        topic, topic_order = self.__sorted(topic[:, newaxis])
        return self.__raveled(topic, topic_order)

    @staticmethod
    def __sorted(array: ndarray) -> (ndarray, ndarray):
        """Sort an array by its first axis in descending order."""
        sorting_indices = (-array).argsort(axis=0)
        return array[sorting_indices, arange(array.shape[1])], sorting_indices.T

    @staticmethod
    def __raveled(*arrays: ndarray) -> Tuple[ndarray, ...]:
        """Ravel several arrays at once."""
        return tuple(array.ravel() for array in arrays)

    @staticmethod
    def __zipped(first: ndarray, second: ndarray) -> Callable[[int], zip]:
        """Return function that zips a 1-D array with a column of a 2D array."""

        def zipped(topic: int) -> zip:
            """Zip a 1-D array with the specified column of a 2-D array."""
            return zip(first[topic], second[:, topic])

        return zipped

    @staticmethod
    def __tuples(vocabulary: Dict[int, str]) -> Callable[[zip], TuplesT]:
        """Return a function that produces (word, importance) tuples."""

        def tuples(zipped: zip) -> TuplesT:
            """Produce (word, importance) tuples."""
            return tuple((vocabulary[index], proba) for index, proba in zipped)

        return tuples

    @staticmethod
    def __non_empty_string(doc: str) -> str:
        doc = str(doc)
        if not doc:
            raise ValueError('Cannot assign topics to an empty string!')
        return doc

    def __safely_processed(self, doc: str) -> Tuple[str, ...]:
        processed = self.__corpus.pipeline.process(doc)
        if not processed:
            raise ValueError('No words left in document after preprocesing!')
        return processed

    @staticmethod
    def __evaluated(encoded: ndarray) -> (ndarray, int):
        known_word_counts, n_new_words = encoded[:-1], int(encoded[-1])
        if known_word_counts.sum() < 1.0:
            raise ValueError("There aren't any known words in the document, "
                             "so no topic weights can be assigned to it.")
        return known_word_counts, n_new_words
